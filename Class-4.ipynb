{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# MA934 - class 4\n",
    "\n",
    "## Deadline: 12:00 noon 29 October 2020\n",
    "\n",
    "## All questions are worth equal marks\n",
    "\n",
    "You should make at least one commit to your repository per computational task below - usually more.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Task 1 - Solving a simple linear programme\n",
    "\n",
    "Consider the following linear programme\n",
    "\n",
    "$$\\min_{\\substack{(x_1, x_2) \\in \\mathbb{R}^2} } -40\\, x_1 - 60\\, x_2$$\n",
    "\n",
    "subject to the constraints\n",
    "\n",
    "$$2\\, x_1 + x_2 \\leq 70 $$\n",
    "$$x_1 + 3\\, x_2 \\leq 90 $$\n",
    "$$ 3\\, x_1 + x_2 \\geq 46 $$\n",
    "$$ x_1 + 4\\, x_2 \\geq 52 $$\n",
    "\n",
    "with $x_1 \\geq 0$ and $x_2 \\geq 0$.\n",
    "\n",
    "Sketch the feasible set for this problem.\n",
    "\n",
    "Determine the coordinates of the vertices of the feasible set in $\\mathbb{R}^2$ and thereby determine the solution of the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2 - Dantzig simplex algorithm\n",
    "\n",
    "Write the above problem in standard form. Find a basic feasible vector in $\\mathbb{R}^6$ with $x_1 = 12$ and $x_2 = 10$.\n",
    "\n",
    "Write a code in Julia that implements the Dantzig simplex algorithm in its simplest form.\n",
    "\n",
    "Start your code from the basic feasible vector that you found above and write down the sequence of basic feasible vectors leading to the solution you found previously.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3 - Gradient descent\n",
    "\n",
    "Consider the following optimisation problem in $\\mathbb{R}^n$\n",
    "\n",
    "$$\\min_{\\substack{\\mathbf{x} \\in \\mathbb{R}^n} } f(\\mathbf{x})$$\n",
    "\n",
    "where $f(\\mathbf{x})$ is the quartic function defined as\n",
    "\n",
    "$$ f(\\mathbf{x}) = \\frac{1}{4} \\left( (\\mathbf{x} - \\mathbf{x}_*)^T A\\cdot (\\mathbf{x} - \\mathbf{x}_*)\\right)^2 $$\n",
    "\n",
    "with \n",
    "\n",
    "$$ A = \\left( \n",
    "\\begin{array}{ccccc} \n",
    "\\frac{\\lambda+1}{2} & \\frac{\\lambda - 1}{2} & 0 &\\ldots & 0\\\\ \n",
    "\\frac{\\lambda-1}{2} & \\frac{\\lambda + 1}{2} & 0 &\\ldots & 0\\\\\n",
    "0 & 0 & 1 &\\ldots & 0\\\\ \n",
    "\\vdots & \\vdots & \\vdots &\\ddots & \\vdots\\\\ \n",
    "0 & 0 & 0 &\\ldots & 1\n",
    "\\end{array}\n",
    "\\right),\n",
    "$$\n",
    "$$\n",
    "\\mathbf{x}_* = \\left( \\begin{array}{c}\n",
    "\\sqrt{2}\\\\\n",
    "\\sqrt{3}\\\\\n",
    "1\\\\\n",
    "\\vdots\\\\\n",
    "1\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "and $\\lambda > 0$ is a parameter that controls the shape of the objective function. A reasonable range of values for $\\lambda$ is $\\frac{1}{10} \\leq \\lambda \\leq 10$.\n",
    "\n",
    "* Write down the solution of this problem.\n",
    "* Write down the eigenvalues of the matrix $A$.\n",
    "* Derive a formula for the gradient, $\\nabla f(\\mathbf{x})$, at any point in $\\mathbb{R}^n$.\n",
    "* Use your results to implement the gradient descent algorithm in Julia. Use your algorithm to numerically solve the above problem with $n=2$ and $\\lambda=2$. A good initial point is $\\mathbf{x}_0 = 5\\,\\mathbf{x}_*$.  A good tolerance is $\\varepsilon = 10\\, \\epsilon_m$. You can use the Golden Section Search code in https://github.com/colm-connaughton/MA934-gss (or write your own).\n",
    "* Plot how the distance from the minimum decreases as s function of the number of iterations and empirically determine the convergence rate of the algorithm.\n",
    "* Determine empirically how the number of steps required to solve the problem varies with $\\lambda$ when $n=3$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4 - Gradient descent using automatic differentiation\n",
    "\n",
    "Julia supports dual numbers via the DualNumbers.jl package. Modify your code from the previous task to do all computations using dual arithmetic and create an implementation of the gradient descent algorithm that uses automatic differentiation to calculate gradients. Check that you get the same answer.\n",
    "\n",
    "Note that calculating partial derivatives using automatic differentiation requires multiple function evaluations. Here's one way to do it for a function, $f(x)$ of $n$ variables:\n",
    "\n",
    "```\n",
    "D = Matrix{Dual}(Dual(0.0,1.0)I, n, n)\n",
    "F = [ f(x + D[:,k]) for k in 1:n]\n",
    "value = realpart(F[1])\n",
    "grad = epsilon.(F)\n",
    "```\n",
    "\n",
    "This assumes that the function $f : \\mathbb{R}^n \\to \\mathbb{R}$ has already been defined, that $x$ is an n-vector\n",
    "of dual numbers with zero dual part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5 - Stochastic gradient descent\n",
    "\n",
    "Consider the following linear model expressing a noisy relationship between a target variable, $y \\in \\mathbb{R}$ and a set of predictor variables, $\\mathbf{x} \\in \\mathbb{R}^n$:\n",
    "\n",
    "$$ y = \\mathbf{\\alpha}^T \\, \\mathbf{x} + \\xi $$\n",
    "\n",
    "where $\\mathbf{\\alpha} \\in \\mathbb{R}^n$ is a set of parameters and $\\xi \\sim N(0, \\sigma^2)$ is a normal random variable with mean 0 and variance $\\sigma^2$ representing the error. We are given a set of $m$ observations\n",
    "\n",
    "$$Y = \\left\\{(\\mathbf{x}^{(i)},\\, y^{(i)})\\ : i=1\\ldots m \\right\\}$$\n",
    "\n",
    "Our task is to find the \"best\" set of parameters, $\\mathbf{\\alpha}_*$ given the observations by solving the ordinary least squares problem:\n",
    "\n",
    "$$\\mathbf{\\alpha}_* = \\min_{\\substack{\\mathbf{\\alpha} \\in \\mathbb{R}^n} } F(\\mathbf{\\alpha}\\, |\\, Y)$$\n",
    "\n",
    "where\n",
    "\n",
    "$$ F(\\mathbf{\\alpha}\\, |\\, Y) = \\frac{1}{2}\\, \\frac{1}{m} \\sum_{i=1}^m\\left( y^{(i)} - \\mathbf{\\alpha}^T\\,\\mathbf{x}^{(i)}\\right)^2$$\n",
    "\n",
    "Create a test problem as follows (obviously you can vary the parameters):\n",
    "\n",
    "```\n",
    "rng = RandomDevice()\n",
    "n=10\n",
    "m=100\n",
    "xmax = 10.0\n",
    "σ = 0.25\n",
    "αstar = reshape(rand(rng, n), n, 1)\n",
    "x = reshape(xmax*rand(rng, n, m), n, m)\n",
    "ξ = reshape(σ*randn(rng, m), 1, m)\n",
    "y = αstar' * x + ξ\n",
    "```\n",
    "\n",
    "* Solve the problem using the gradient descent code you have written above. Due to the noise, you should not expect to recover the exact \"true\" value of $\\alpha_*$ used to generate the test data but you should be close if the noise is not too large.\n",
    "* Modify your code to do the optimisation using stochastic gradient descent and compare the results graphically.\n",
    "* Fix $n=10$ and $m=250$. Compare the performance of your stochastic gradient descent algorithm with different learning rates. Can you find one that performs well?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.1",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
